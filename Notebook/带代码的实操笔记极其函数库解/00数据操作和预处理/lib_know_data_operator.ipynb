{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lib库解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先导个库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理操作基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n           = 12 #用来做下文数据代指的\n",
    "tensor_hang = 3  #用来进行行指代\n",
    "tensor_lie  = 4  #用来进行列指代\n",
    "tensor_zong = 2  #用来进行纵指代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在torch中正确的开辟新不相关数组的方法！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个初始的x_1\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "这是把x_1.T的值赋给x_2(我们想这么做),然后修改x_2[0,0]的值为1之后x_1矩阵的状态\n",
      "tensor([[ 1,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "这是你新开了地址之后再赋值然后修改x_3[0,0]的值为100之后x_1的情况\n",
      "tensor([[100,   1,   2,   3],\n",
      "        [  4,   5,   6,   7],\n",
      "        [  8,   9,  10,  11],\n",
      "        [ 12,  13,  14,  15],\n",
      "        [ 16,  17,  18,  19]])\n",
      "这是原本的x_4\n",
      "tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]])\n",
      "这是x_4修改值前的x_1\n",
      "tensor([[100,   1,   2,   3],\n",
      "        [  4,   5,   6,   7],\n",
      "        [  8,   9,  10,  11],\n",
      "        [ 12,  13,  14,  15],\n",
      "        [ 16,  17,  18,  19]])\n",
      "这是修改x_4[0,0]值为250后的x_1\n",
      "tensor([[100,   1,   2,   3],\n",
      "        [  4,   5,   6,   7],\n",
      "        [  8,   9,  10,  11],\n",
      "        [ 12,  13,  14,  15],\n",
      "        [ 16,  17,  18,  19]])\n"
     ]
    }
   ],
   "source": [
    "x_1 = tor.arange(20).reshape(5,4)\n",
    "print(\"这是一个初始的x_1\")\n",
    "print(x_1)\n",
    "x_2 = x_1.T # 这是一个x_1的转置矩阵，被赋给x_2，在C中，这会使得两个数据不相关了，当然，C中需要创建一个新的x_2数组并且和要赋给的x_1.T数据类型和结构一致\n",
    "x_2[0,0] = 1\n",
    "print(\"这是把x_1.T的值赋给x_2(我们想这么做),然后修改x_2[0,0]的值为1之后x_1矩阵的状态\")\n",
    "print(x_1) # 看哪！发生了什么！我们原本知识想修改x_2的值但是x_1也被改了\n",
    "'''\n",
    "这就不得不提到python的 view 机制了\n",
    "python为了缓解减少内存的占用,所以会衍生出一种机制,称为\"view\"\n",
    "这一个机制的目的旨在减少内存占用,所以当一个变量在没有进行初始化(规定构型)的情况下被进行将一个值的数据给到它,它就会变成一个view(同一个地址开两个变量名,有点像C中编写函数时传入形参是个地址的话那么函数中的形参名其实就是主程序中同一个地址的实参的别名,C++中有类似应用,但是在python中你并不知道它是别名,而会认为是开了新地址),而非是去新开设一个内存地址空间.\n",
    "所以,无形中这个变量就产生了同一个地址多个变量名绑定,这是致命的,修改view的值就是修改原始变量的值!\n",
    "'''\n",
    "# 听了上面的陈述,你知道了结症所在然后你先开了个带有一定构型的变量再赋值\n",
    "x_3 = tor.zeros_like(x_1.T)\n",
    "x_3 = x_2\n",
    "x_3[0,0] = 100\n",
    "print(\"这是你新开了地址之后再赋值然后修改x_3[0,0]的值为100之后x_1的情况\")\n",
    "print(x_1) # oh shit 居然又错了,为什么呢?其实你离成功只差一步,你可以做一个实验就是将不符合结构要求的值赋给x_3你看看会发生什么\n",
    "x_3 = x_1 # amazing!你警觉了,如果是真的像C一样是开了一个新地址的操作,你在赋给x_3不符合它构型的x_1!(因为x_3如果按你构建的时候来的话应该是Size(4,5)构型而Size(5,4)构型的x_1根本进不来)\n",
    "# 你顿悟了,你被骗了,python其实还是给你创了个view,尼玛的,那么正确的应该怎么做呢?\n",
    "x_4= tor.zeros_like(x_1);# 这里其实是创建了新的地址空间的\n",
    "print(\"这是原本的x_4\")\n",
    "print(x_4)\n",
    "x_4[:] = x_1;\n",
    "print(\"这是x_4修改值前的x_1\")\n",
    "print(x_1)\n",
    "x_4[0,0] = 250\n",
    "print(\"这是修改x_4[0,0]值为250后的x_1\")\n",
    "print(x_1)# ohhhhhhhhhhhhhhhhh 你成功了[:]其实就是一个只修改变量值而不改地址的操作，nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_operator_basic = tor.arange(n)\n",
    "x_data_operator_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：arange(n)\n",
    "- 函数作用：是填入数据n生成一个从0~n-1的张量\n",
    "- 函数输入：数字 n\n",
    "- 函数输出：torch的张量（1 x n）\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_operator_basic.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：numel()\n",
    "- 函数作用：输出torch_tensor含有的所有元素的总数\n",
    "- 函数输入：无（依赖型）\n",
    "- 函数输出：标量数字\n",
    "- 函数依赖：是--依赖torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_operator_basic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：shape\n",
    "- 函数作用：输出torch_tensor的结构\n",
    "- 函数输入：无（依赖型）\n",
    "- 函数输出：torch.size()【这是一个会描述torch_tensor结构的数据类型】\n",
    "- 函数依赖：是--依赖torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_operator_basic.reshape(tensor_hang,tensor_lie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：reshape(行,列)\n",
    "- 函数作用：将torch_tensor在元素不变的情况下结构发生改变，变为 n行 m列\n",
    "- 函数输入：tensor_hang 要改变成的张量的行 tensor_lie 要改变成的张量的列\n",
    "- 函数输出：变化后的张量\n",
    "- 函数依赖：是--依赖torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tor.zeros((tensor_zong,tensor_hang,tensor_lie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：zeros((张量结构))\n",
    "- 函数作用：生成全零的张量\n",
    "- 函数输入：张量的结构\n",
    "- 函数输出：全零的特定结构的torch_tensor\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tor.ones((tensor_zong,tensor_hang,tensor_lie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：ones((张量结构))\n",
    "- 函数作用：生成全1的张量\n",
    "- 函数输入：张量的结构\n",
    "- 函数输出：全1的特定结构的torch_tensor\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7816, -0.0295, -0.6303, -0.8054],\n",
       "         [-0.9329,  0.9743,  0.4026, -1.5266],\n",
       "         [ 2.0828, -0.0584, -0.9778,  0.7083]],\n",
       "\n",
       "        [[-0.5941,  0.2936, -1.1415,  1.0712],\n",
       "         [ 0.4574, -0.0423,  0.2291, -1.3703],\n",
       "         [-0.6084,  0.5535,  1.6587,  0.4079]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tor.randn((tensor_zong,tensor_hang,tensor_lie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：randn((张量结构))\n",
    "- 函数作用：生成全任意数<0~1>的张量\n",
    "- 函数输入：张量的结构\n",
    "- 函数输出：全任意数<0~1>的特定结构的torch_tensor\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tor.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：tensor([张量详细结构及数据])  【数据书写方式为一个方括号为一层，方括号中只能填写元素或包裹其他方括号，方括号中若为元素，表示的是列元素。如果是行的话就是逗号分隔两个内含元素的方括号；如果是纵的话就是逗号分隔两个是行的方括号；以此类推】\n",
    "- 函数作用：生成你自己设计的torch_tensor\n",
    "- 函数输入：自己设计的张量的结构和数据\n",
    "- 函数输出：自己设计的特定结构的torch_tensor\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python数组操作基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(11), tensor([1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_operator_basic[-1],x_data_operator_basic[1 : 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 这是一个python的基础使用 -1 是倒数第一个**这一层的元素**（可能不是直接是数字哦）\n",
    "> \n",
    "> 分号是这个区间内的**这一层的元素**1:2是只有1，大的那个不会出现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 100,  12,  12,  12,   5, 100,   7,   8,   9,  10,  11])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_operator_basic[1] = 19\n",
    "x_data_operator_basic[2 : 5] = 12\n",
    "x_data_operator_basic[1 : 11 : 5] = 100\n",
    "x_data_operator_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 这是一个元素修改的问题\n",
    "> \n",
    "> 其实就是逗号就是分层（维度）\n",
    ">\n",
    "> 其实分号就是本层区块操作\n",
    ">\n",
    "> (hang_head):(hang_tail):step 双分号后面那个就是每隔多少一读"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00,        inf, 1.6275e+05, 1.6275e+05, 1.6275e+05, 1.4841e+02,\n",
       "               inf, 1.0966e+03, 2.9810e+03, 8.1031e+03, 2.2026e+04, 5.9874e+04])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tor.exp(x_data_operator_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：exp(torch_tensor)\n",
    "- 函数作用：生成一个与输入的torch_tensor一样结构的张量，但是张量里的数字全变为指数项\n",
    "- 函数输入：torch_tensor\n",
    "- 函数输出：$e^{torch\\_tensor}$当然这也是一个张量\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch元素级元素运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tor.tensor([1.0, 2, 4, 8])\n",
    "y = tor.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|元素级运算符|解释|\n",
    "|--|--|\n",
    "| **+** | 元素相加 |\n",
    "| **-** | 元素相减 |\n",
    "| **\\*** | 元素相乘 |\n",
    "| **/** | 元素相除 |\n",
    "| **\\*\\*** | 元素指数操作 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.],\n",
       "         [ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.,  0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.,  4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.,  8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tor.arange(12, dtype=tor.float32).reshape((3,4))\n",
    "Y = tor.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "Z = tor.arange(12, dtype=tor.float32).reshape((3,4))\n",
    "tor.cat((X, Y, Z), dim=0), tor.cat((X, Y, Z), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：cat((torch_tensor1, torch_tensor2, torch_tensor3……), dim = 层数)\n",
    "- 函数作用：将输入的torch_tensor依据编写的dim层数数据在相应层次上拼接\n",
    "- 函数输入：(torch_tensor1,torxh_tensor2……), dim = n\n",
    "- 函数输出：拼接好的torch_tensor\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 小提示\n",
    ">\n",
    "> arange 可以依据 dtype参数的强制修改进行数据格式的强制修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch简单生成正则矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是依据相等符进行张量兵对兵将对将的二元比较形成一个新的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(286)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_operator_basic.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：sum()\n",
    "- 函数作用：将依赖的torch_tensor进行数值求和\n",
    "- 函数输入：无\n",
    "- 函数输出：所有元素的求和\n",
    "- 函数依赖：是--依赖torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zeros_like && ones_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_imitation1 = tor.zeros_like(x_data_operator_basic)\n",
    "x_imitation2 = tor.ones_like(x_data_operator_basic)\n",
    "x_imitation1, x_imitation2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：zeros_like(torch_tensor)\n",
    "- 函数作用：生成一个与torch_tensor张量类似结构的全0张量\n",
    "- 函数输入：torch_tensor\n",
    "- 函数输出：torch_tensor结构与输入的torch_tensor相似的全0张量\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：ones_like(torch_tensor)\n",
    "- 函数作用：生成一个与torch_tensor张量类似结构的全1张量\n",
    "- 函数输入：torch_tensor\n",
    "- 函数输出：torch_tensor结构与输入的torch_tensor相似全1张量\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id(内存查询)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1691783470752"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(x_data_operator_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数名字：id(变量)\n",
    "- 函数作用：读取变量在内存中的地址\n",
    "- 函数输入：变量名\n",
    "- 函数输出：变量在内存中的地址\n",
    "- 函数依赖：否"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编程错误注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量结构不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tor.arange(3).reshape((3, 1))\n",
    "b = tor.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种大小不一致的两个张量他们会补0补齐，注意不要这样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 无意中的新内存申请"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种其实就是搞了一个锁存器，但是其实由于我们在进行高层次语言编写的时候编译器会自动将等号右边和左边分割开所以进行新内存的索取，就导致他们没有认为这是一个锁存器结构，导致我们无意间就开了一个新的空间。当然这种问题是因为python抽像层次太高导致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = X + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然大家会好奇为什么这样也没有幸免呢，其实是因为python为了追求自己的编写语法可以极为随意所造成的恶果，为了保证我们数据可以接受任意的类型的数据(python数据类型极为不敏感)，所以与其担心数据会类型不匹配，直接就开一个新的内存地址。这看起来无可厚非，但这会导致内存的极大浪费（做傻子的代价）【C是全球最好的语言，C++也不错^_^】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 解决办法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个办法其实是有的，C就不会莫名自己申请内存，为什么呢？这其实就是我们自己定义了数据的类型，他知道我们只会用那么多，那个东西，所以没必要给我们开新的地址。\n",
    "\n",
    "这下好解决了，就是要让python知道自己的类型结构没有改变，改变的只有数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 1691785458048\n",
      "id(Z): 1691785458048\n"
     ]
    }
   ],
   "source": [
    "Z = tor.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看我们成功了，我们通过 **[:]** 我们向python说明了我们只是要填写数值，并不是要直接把结构都变了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X[:] = Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X[:] = X + Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看我们成功了，这就是python的不偷内存的写法，最上面那个是简单写法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy是爹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无论是torch还是tensorflow还是别的什么，在python中用数组（就是前面我们提到的变量的一种叫法）的，都得叫numpy一声爹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = tor.from_numpy(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一种torch转换，转换numoy为torch_tensor结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tor.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据转换，就和C/C++一样"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
