{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example 1 hidden layer MLP(但隐藏层多层感知机)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tor\n",
    "from torch import nn as tor_nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0060],\n",
       "        [0.0993]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tor_nn.Sequential(\n",
    "    tor_nn.Linear(4, 8),tor_nn.ReLU(),\n",
    "    tor_nn.Linear(8, 1)\n",
    ")\n",
    "X = tor.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequential 可以简单的理解为python中的表单(list)\n",
    "\n",
    "因此我们可以通过一些办法去访问这个表单，就像我们使用索引去访问数组一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.1457,  0.4638, -0.1093,  0.1620],\n",
      "        [ 0.3534, -0.0251, -0.2591,  0.4670],\n",
      "        [-0.4415, -0.0902,  0.0551, -0.3649],\n",
      "        [ 0.2536, -0.3261,  0.4174, -0.2729],\n",
      "        [-0.2054, -0.3219, -0.1962, -0.3010],\n",
      "        [-0.2352, -0.0879,  0.3241,  0.1783],\n",
      "        [ 0.2218, -0.3994, -0.3758,  0.3522],\n",
      "        [ 0.0490, -0.0114, -0.0968,  0.3093]])), ('bias', tensor([-0.4103, -0.0907, -0.2448, -0.4558, -0.0382, -0.0858, -0.3601,  0.2401]))])\n",
      "OrderedDict()\n",
      "OrderedDict([('weight', tensor([[ 0.0421, -0.1938,  0.3524,  0.2961, -0.1071,  0.1066,  0.1025, -0.2362]])), ('bias', tensor([0.2057]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[0].state_dict())\n",
    "print(net[1].state_dict())\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到我们的计数是从0开始的，然后到2，其中我们有:\n",
    "- 第一层Linear(4, 8)\n",
    "- 第二层ReLU(这玩意没什么参数)\n",
    "- 第三层Linear(8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2057], requires_grad=True)\n",
      "tensor([0.2057])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias)) # 类型\n",
    "print(net[2].bias)       # 详细参数\n",
    "print(net[2].bias.data)  # 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉就是一个C的链表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0421, -0.1938,  0.3524,  0.2961, -0.1071,  0.1066,  0.1025, -0.2362]],\n",
      "       requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net[2].weight)\n",
    "print(net[2].weight.grad) # 这里我们看到是None应为我们话没有进行反向传播所以是None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单个第0层参数\n",
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "多层所有参数\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(\"单个第0层参数\")\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(\"多层所有参数\")\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "名字作为索引访问(感觉像Lua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0421, -0.1938,  0.3524,  0.2961, -0.1071,  0.1066,  0.1025, -0.2362]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.weight'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1():\n",
    "    return tor_nn.Sequential(\n",
    "        tor_nn.Linear(4, 8), tor_nn.ReLU(),\n",
    "        tor_nn.Linear(8, 4), tor_nn.ReLU()\n",
    "    )\n",
    "\n",
    "def block2():\n",
    "    net = tor_nn.Sequential() # 表明类型创建一个空表，你可以看成一个类型指代\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}', block1()) # add_module的作用是我们可以去编排net中层的名字，就不用是0~n了\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0642],\n",
       "        [-0.0642]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet = tor_nn.Sequential(block2(), tor_nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展示所有这个网络的构成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-limurequire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
