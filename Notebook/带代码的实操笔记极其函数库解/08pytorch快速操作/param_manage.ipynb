{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example 1 hidden layer MLP(但隐藏层多层感知机)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tor\n",
    "from torch import nn as tor_nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0913],\n",
       "        [-0.1703]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tor_nn.Sequential(\n",
    "    tor_nn.Linear(4, 8),tor_nn.ReLU(),\n",
    "    tor_nn.Linear(8, 1)\n",
    ")\n",
    "X = tor.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequential 可以简单的理解为python中的表单(list)\n",
    "\n",
    "因此我们可以通过一些办法去访问这个表单，就像我们使用索引去访问数组一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.0188,  0.3250,  0.4889, -0.3333],\n",
      "        [ 0.4175, -0.4646,  0.0190,  0.3603],\n",
      "        [ 0.3477, -0.1676, -0.4880,  0.1333],\n",
      "        [-0.2342,  0.0826,  0.0449, -0.3643],\n",
      "        [ 0.3214,  0.1281, -0.4591,  0.4121],\n",
      "        [ 0.1273, -0.3304,  0.4242,  0.0339],\n",
      "        [ 0.3894,  0.0007, -0.5000,  0.0784],\n",
      "        [-0.4300,  0.2715,  0.2679,  0.0498]])), ('bias', tensor([-0.1399, -0.3402, -0.4977, -0.2135,  0.4336,  0.3656,  0.2576,  0.2798]))])\n",
      "OrderedDict()\n",
      "OrderedDict([('weight', tensor([[ 0.3208, -0.1108,  0.2642,  0.0568, -0.3127, -0.2750,  0.3423, -0.0592]])), ('bias', tensor([0.2030]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[0].state_dict())\n",
    "print(net[1].state_dict())\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到我们的计数是从0开始的，然后到2，其中我们有:\n",
    "- 第一层Linear(4, 8)\n",
    "- 第二层ReLU(这玩意没什么参数)\n",
    "- 第三层Linear(8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2030], requires_grad=True)\n",
      "tensor([0.2030])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias)) # 类型\n",
    "print(net[2].bias)       # 详细参数\n",
    "print(net[2].bias.data)  # 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉就是一个C的链表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3208, -0.1108,  0.2642,  0.0568, -0.3127, -0.2750,  0.3423, -0.0592]],\n",
      "       requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net[2].weight)\n",
    "print(net[2].weight.grad) # 这里我们看到是None应为我们话没有进行反向传播所以是None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单个第0层参数\n",
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "多层所有参数\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(\"单个第0层参数\")\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(\"多层所有参数\")\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "名字作为索引访问(感觉像Lua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3208, -0.1108,  0.2642,  0.0568, -0.3127, -0.2750,  0.3423, -0.0592]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.weight'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1():\n",
    "    return tor_nn.Sequential(\n",
    "        tor_nn.Linear(4, 8), tor_nn.ReLU(),\n",
    "        tor_nn.Linear(8, 4), tor_nn.ReLU()\n",
    "    )\n",
    "\n",
    "def block2():\n",
    "    net = tor_nn.Sequential() # 表明类型创建一个空表，你可以看成一个类型指代\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}', block1()) # add_module的作用是我们可以去编排net中层的名字，就不用是0~n了\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1057],\n",
       "        [0.1056]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet = tor_nn.Sequential(block2(), tor_nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展示所有这个网络的构成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0028,  0.0044,  0.0108,  0.0009]), tensor(0.))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if(type(m) == tor_nn.Linear):\n",
    "        tor_nn.init.normal_(m.weight, mean=0, std=0.01) # 正态分布均值为0，方差0.01 |normal_是一种pytorch的写法，就是直接替换\n",
    "        tor_nn.init.zeros_(m.bias)                      # b参数全归0\n",
    "        \n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == tor_nn.Linear:\n",
    "        tor_nn.init.constant_(m.weight, 1) # 直接固定为确信值\n",
    "        tor_nn.init.zeros_(m.bias)\n",
    "        \n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "某些应用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5562, -0.3808,  0.1112, -0.0838])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if(type(m) == tor_nn.Linear):\n",
    "        tor_nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "def init_42(m):\n",
    "    if(type(m) == tor_nn.Linear):\n",
    "        tor_nn.init.constant_(m.weight, 42)\n",
    "        \n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里，apply就是表示加入进入模型并且直接进行对应运行的操作，apply在加入的时候就已经进行了运行。很符合一种插入过程之中的感觉（就像往三明治里面加肉加菜）！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -7.6605,  0.0000, -0.0000],\n",
       "        [-0.0000, -8.1597, -0.0000,  9.8050]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if(type(m) == tor_nn.Linear):\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[(name, param.shape) for name, param in m.named_parameters()][0] # [0]是weight, [1]是bias\n",
    "        )\n",
    "        tor_nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "        \n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一种就是直接做操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[42.0000, -6.6605,  1.0000,  1.0000],\n",
       "         [ 1.0000, -7.1597,  1.0000, 10.8050],\n",
       "         [10.1559,  1.0000,  1.0000,  1.0000],\n",
       "         [-8.8623,  1.0000,  1.0000,  7.7619],\n",
       "         [-7.2341,  1.0000,  1.0000,  9.6321],\n",
       "         [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [ 1.0000,  1.0000,  9.7660,  8.1742],\n",
       "         [ 6.3477,  1.0000, -8.2250,  1.0000]]),\n",
       " tensor([42.0000, -6.6605,  1.0000,  1.0000]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data, net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数绑定(共享权重)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实想就是说我们的参数1与另一个参数变为一致的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = tor_nn.Linear(8, 8)\n",
    "net = tor_nn.Sequential(\n",
    "    tor_nn.Linear(4, 8), # 0\n",
    "    tor_nn.ReLU(),       # 1\n",
    "    shared,              # 2\n",
    "    tor_nn.ReLU(),       # 3\n",
    "    shared,              # 4\n",
    "    tor_nn.ReLU(),       # 5\n",
    "    tor_nn.Linear(8, 1)  # 6\n",
    ")\n",
    "\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-limurequire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
