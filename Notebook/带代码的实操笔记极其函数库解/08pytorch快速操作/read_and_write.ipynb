{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件读写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载和保存张量(矩阵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tor\n",
    "from torch import nn as tor_nn\n",
    "from torch.nn import functional as tor_nn_F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tor.arange(4)\n",
    "tor.save(x, 'x-file')\n",
    "\n",
    "x2 = tor.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存储一个张量列表，然后将他们读回内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tor.zeros(4)\n",
    "tor.save([x, y], 'xy-file') # 这里就可以看到度的是一个列表\n",
    "x2, y2 = tor.load('xy-file')\n",
    "\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用python的字典对于数据进行特殊的索引标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = {'x' : x, 'y' : y}\n",
    "tor.save(my_dict, 'my_dict')\n",
    "\n",
    "my_dict2 = tor.load('my_dict')\n",
    "my_dict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载与保存模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到我们加载和保存模型参数，由于模型并不是一个单独的矩阵，并且他们有一定的模型结构，所以会才有别的方式去搭建模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_RW(tor_nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = tor_nn.Linear(20, 256)\n",
    "        self.output = tor_nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output(tor_nn_F.relu(self.hidden(x)))\n",
    "    \n",
    "net = MLP_RW()\n",
    "X = tor.randn(size = (2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就是将模型的参数存入到一个后缀为 .params 的文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 0.1862,  0.1383, -0.1819,  ...,  0.1951,  0.1637,  0.0874],\n",
       "                      [-0.0618, -0.0119,  0.0510,  ..., -0.1947, -0.1041, -0.0753],\n",
       "                      [ 0.1732,  0.0181, -0.1076,  ...,  0.1266, -0.0324,  0.0499],\n",
       "                      ...,\n",
       "                      [-0.0564,  0.2200,  0.2235,  ...,  0.1828,  0.1041,  0.1561],\n",
       "                      [-0.1013,  0.1492,  0.1231,  ...,  0.2062,  0.1642,  0.1825],\n",
       "                      [ 0.1044, -0.1886, -0.0864,  ...,  0.2089,  0.0791,  0.0268]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([-0.1690, -0.0954, -0.2136, -0.0375,  0.2088,  0.0008,  0.1445, -0.0699,\n",
       "                      -0.0770,  0.0155,  0.1877,  0.2126, -0.1203,  0.0236, -0.1534, -0.0337,\n",
       "                       0.1316,  0.0214, -0.0153,  0.0405, -0.1814, -0.1155,  0.1642,  0.1430,\n",
       "                      -0.1244, -0.1034, -0.1084,  0.0922,  0.0287, -0.2212,  0.1513,  0.1089,\n",
       "                      -0.1262, -0.1888,  0.2218,  0.1106, -0.0620, -0.1878,  0.1064, -0.0140,\n",
       "                       0.0155, -0.1664, -0.0589, -0.0527, -0.0357, -0.2010, -0.1713, -0.0743,\n",
       "                      -0.0246, -0.0521,  0.1084, -0.1731,  0.0649,  0.1431,  0.1690,  0.0041,\n",
       "                      -0.1353,  0.0533, -0.2134, -0.1412,  0.2105,  0.2164,  0.1284, -0.0811,\n",
       "                       0.1139,  0.0332, -0.1413, -0.2145, -0.1136, -0.0871, -0.1242, -0.1338,\n",
       "                       0.0753, -0.1350, -0.1584, -0.0939, -0.0476, -0.0572, -0.0116, -0.0418,\n",
       "                       0.1959, -0.0449,  0.2211, -0.0267, -0.1640, -0.0747, -0.0367,  0.0410,\n",
       "                      -0.1923, -0.2206,  0.1577, -0.1189,  0.1383, -0.1411,  0.2223, -0.0130,\n",
       "                       0.2180, -0.0899,  0.1082,  0.2113,  0.1939, -0.1305, -0.1301,  0.0659,\n",
       "                       0.0332,  0.1337,  0.1872, -0.2214,  0.1487, -0.1203, -0.0851,  0.1998,\n",
       "                       0.0808, -0.0071,  0.0499, -0.1936, -0.0136,  0.0548,  0.2067,  0.0398,\n",
       "                       0.1489, -0.0739,  0.2192,  0.1715,  0.1021, -0.0638,  0.1854, -0.0280,\n",
       "                       0.1490,  0.1589, -0.0753, -0.1542, -0.1879,  0.1842, -0.0746,  0.1227,\n",
       "                       0.0316, -0.0796,  0.0196,  0.1741, -0.0387,  0.1702,  0.0231,  0.0648,\n",
       "                       0.0456, -0.0166, -0.2149, -0.0525,  0.1931, -0.2093, -0.1782, -0.1568,\n",
       "                      -0.1941,  0.0045, -0.0567,  0.0815,  0.0554, -0.0674, -0.1082,  0.0482,\n",
       "                      -0.0486,  0.0097,  0.1636, -0.1701,  0.1488, -0.1444, -0.1510,  0.1549,\n",
       "                      -0.0631, -0.1956,  0.1626,  0.1951, -0.0613, -0.0631, -0.1791,  0.2190,\n",
       "                      -0.1148, -0.0028,  0.2162,  0.1506,  0.1209,  0.0173, -0.0040,  0.1784,\n",
       "                       0.1430, -0.0299,  0.0718,  0.0262, -0.1925, -0.1746, -0.0359, -0.1867,\n",
       "                      -0.2113, -0.0676, -0.0507,  0.0623,  0.2126,  0.1215, -0.1104,  0.0057,\n",
       "                      -0.0762,  0.0295, -0.2191, -0.0934, -0.1071,  0.0425,  0.1897, -0.0370,\n",
       "                      -0.2072,  0.1970, -0.0989,  0.2156, -0.0779, -0.1801, -0.0390, -0.2043,\n",
       "                      -0.1396,  0.1737,  0.2079, -0.0938,  0.1216, -0.0561,  0.0749, -0.1702,\n",
       "                       0.1344,  0.1902, -0.1691, -0.0384,  0.1758, -0.1163, -0.0038,  0.1241,\n",
       "                       0.1174, -0.0382, -0.0959, -0.1468,  0.0980,  0.0652,  0.0171,  0.1723,\n",
       "                       0.0676, -0.0877, -0.0118, -0.1344, -0.0611,  0.0382, -0.1055,  0.0229,\n",
       "                      -0.0572, -0.2192,  0.1220,  0.1222,  0.1186,  0.0564,  0.0459,  0.0440])),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.0187,  0.0321,  0.0293,  ...,  0.0344,  0.0250, -0.0372],\n",
       "                      [-0.0002, -0.0586,  0.0407,  ..., -0.0540,  0.0018,  0.0028],\n",
       "                      [-0.0210,  0.0060, -0.0397,  ..., -0.0243,  0.0472,  0.0279],\n",
       "                      ...,\n",
       "                      [-0.0189, -0.0261, -0.0160,  ..., -0.0540,  0.0076, -0.0502],\n",
       "                      [ 0.0137, -0.0062, -0.0270,  ...,  0.0439,  0.0504,  0.0555],\n",
       "                      [-0.0528,  0.0432, -0.0104,  ..., -0.0378, -0.0005, -0.0015]])),\n",
       "             ('output.bias',\n",
       "              tensor([ 0.0360, -0.0176,  0.0367, -0.0126, -0.0219,  0.0497, -0.0120, -0.0262,\n",
       "                       0.0204, -0.0280]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tor.save(net.state_dict(), 'mlp_rw.params')\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是让我们的数据进行一下回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_RW(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone_mod = MLP_RW()\n",
    "clone_mod.load_state_dict(tor.load(\"mlp_rw.params\"))\n",
    "clone_mod.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone_mod(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们其实可以看到，这并不算是一个完全意义上的存模型，这个东西其实是一种参数单独剔除出来，然后骨架是还在那里的，要使用时是先实例化骨架再将参数放入其中。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-limurequire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
